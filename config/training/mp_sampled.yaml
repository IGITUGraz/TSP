defaults:
  - optimizer: rnn_optimizer_mp_sampled
  - transform@state_transform: h_mem_states_transform
  - transform@gradient_transform: h_mem_gradient_stop
  - optimizer@params_transform: mp_sampled_params_transform
  - _self_
runner: rl
experiment_name: match_to_sample_sampled
indep_test_env: false
seed: null
nb_policy_iteration: 315
batch: 32
# 315*32 = 10_080 episodes
loss_log_frequency: 50
eval_frequency: 5
nb_episodes_per_eval: 50
checkpoint_frequency: 0

temporal_type: null
batch_config: null
online_learning: true

loss:
  func: vanilla_policy_gradient
  params:
    vectorize_model: false
    supervised: false
    temperature: 1.0
    gamma: 0.98
    lambda: 0.0
    clip_param: 0.2
    importance_sampling: true
    entropy_coef:
      init_value: 0.9
      transition_steps: 100
      decay_rate: 0.99
      transition_begin: 100

eval_params:
  obs_type: both
accumulator:
  # nb_store_state + hops + 1 (terminal state)
  max_episode_len: 11
  multiple_episodes_in_batch: false